{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Grieve_S_4BC.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEwP4gMtozsC"
      },
      "source": [
        "# Module 4 Guidance\n",
        "\n",
        "This notebook is a template for module 4b and 4c, which will be tested in Google Colab, your code needs to run there.\n",
        "The structure has been provided to improve consistency and make it easier for markers to understand your code but still give students the flexibility to be creative.  You need to populate the required functions to solve this problem.  All dependencies should be documented in the next cell.\n",
        "\n",
        "You can:\n",
        "    add further cells or text blocks to extend or further explain your solution\n",
        "    add further functions\n",
        "\n",
        "Dont:\n",
        "    rename functions\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK3cFfJhozsE",
        "outputId": "bfa5b855-484a-4865-9d7b-780eae5d69e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Fixed dependencies - do not remove or change.\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "# Import your dependencies\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Tuning\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Evaluating\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaiEkZORozsJ"
      },
      "source": [
        "# Import data\n",
        "\n",
        "def import_local_data(file_path):\n",
        "    \"\"\"This function needs to import the data file into collab and return a pandas dataframe\n",
        "    \"\"\"\n",
        "    raw_df = pd.read_excel(file_path)\n",
        "    \n",
        "\n",
        "\n",
        "    return raw_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQzCxqVNozsO",
        "outputId": "31753cd6-d952-417c-8947-4c405fcf99cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The file is hosted on my google drive\n",
        "% cd ../content/gdrive/My Drive\n",
        "local_file = \"breast-cancer.xls\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6OYEsyKozsS"
      },
      "source": [
        "# Dont change\n",
        "data = import_local_data(local_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmB51hH2ozsW"
      },
      "source": [
        "### Conduct exploratory data analysis and explain your key findings - Examine the data, explain its key features and what they look like.  Highlight any fields that are anomalous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-b-mS6wozsX"
      },
      "source": [
        "data.head(10)\n",
        "# Class is the target column. It has a binary value which can easily be changed to numerical. There are some anomolous fields at first glance; there are what appear to be datetime objects in the inv-nodes and tumor-size column.\n",
        "# in addition to Class, node-caps, breast and irradiat are binary categorical values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgVocYC4sxY7"
      },
      "source": [
        "data.info() # ideally all of the categories will have a numerical value, thos that are categorical will need to be labelled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH55ouKvtuue"
      },
      "source": [
        "# a quick check to see if there's any missing data in the columns\n",
        "data.isnull().sum().sort_values(ascending=False)\n",
        "# there is no missing data, but there may be additional spurious values such as the datetime objects noted when looking at head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcMUbM__uC9H"
      },
      "source": [
        "for col in data:\n",
        "    print(data[col].value_counts(),'\\n')\n",
        "\n",
        "# Displaying each data category in this manner to deterine the unique values\n",
        "# Doing this demonstrated that although there is no 'missing' data, there are some suspect values in some columns that will need dealing with separately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRUKxa9r056R"
      },
      "source": [
        "# in the tumor-size and inv-nodes category there are some values showing as datetime objects. These provide no value. To allow me isolate them from the other values for some quick analysis\n",
        "# I ensure they're all converted to strings, as per the other values\n",
        "data.loc[:, 'tumor-size'] = data.loc[:, 'tumor-size'].map(lambda x: str(x))\n",
        "\n",
        "data.loc[:, 'inv-nodes'] = data.loc[:, 'inv-nodes'].map(lambda x: str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf_dRmGwozsg",
        "outputId": "52420eec-2bde-4e96-cf41-cc518a1fc11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# having convereted them to strings I can single out the spurious data as it's a far longer string that the 'correct' data\n",
        "per_tum = data['tumor-size'][data['tumor-size'].str.len()>5].count()/data['tumor-size'].count()*100\n",
        "count_tum = data['tumor-size'][data['tumor-size'].str.len()>5].count()\n",
        "print(f'number of rows with spurious data for tumor-size is: {count_tum}, as a precentage that is {per_tum.round(3)} of the total')\n",
        "# this shows that the spurious data represents just over 11% of all data for this category.\n",
        "\n",
        "per_inv = data['inv-nodes'][data['inv-nodes'].str.len()>5].count()/data['inv-nodes'].count()*100\n",
        "count_inv = data['inv-nodes'][data['inv-nodes'].str.len()>5].count()\n",
        "print(f'number of rows with spurious data for inv-nodes is: {count_inv}, as a percentage that is {per_inv.round(3)} of the total')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of rows with spurious data for tumor-size is: 32, as a precentage that is 11.189 of the total\n",
            "number of rows with spurious data for inv-nodes is: 66, as a percentage that is 23.077 of the total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS4ujhp91Yo-"
      },
      "source": [
        "# how many rows in both tumor-size and 'inv-nodes have spurious data?\n",
        "data[(data['tumor-size'].str.len()>5) & (data['inv-nodes'].str.len()>5)]\n",
        "# I did this to see if it would be reasonable to remove the anomolous data, but of a total of 98 rows with datetime objects as values, only 2 were in both columns in a row."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3nwxWkJ1v_X"
      },
      "source": [
        "*I spent some time here wondering how to best approach this. I didn't want to simply replace the anomolous values with the most common, but equally there was to much spurious data to remove.*\n",
        "\n",
        "*Then I remembered I'd seen this type of 'corruption' of data before in excel where it just decides something it's seeing is a date, when it's not.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDABE1C-1qbS"
      },
      "source": [
        "# I don't know if this happened after I downloaded the file as i viewed it in excel before moving to gdrive. I decided to accept it as a thing to deal with.\n",
        "data['tumor-size'].value_counts()\n",
        "# the two values in tumor-size are most likely 10-14 and 05-09.\n",
        "# it's really obvious now i've noticed it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kQfGxUx2kVn"
      },
      "source": [
        "data['inv-nodes'].value_counts()\n",
        "# I think the values here should be 3-5, 6-8,9-11 and 12-14"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDxQsy9PRq1I"
      },
      "source": [
        "# Explain your key findings\n",
        "*9 contributing data fields and 1 target category\n",
        " all but 1 of the categories are object dtypes which will need to be converted into numerical representations of the data\n",
        " there's no missing data, but there are some anomolous values. In node-caps and breast-quad we have ? showing as a value. It's a small amount of the whole so that will just be assigned as a common value based on the rest of the data\n",
        " in tumor-size and inv-nodes there are a large number of fields showing datetime objects as values. I've concluded this is a corruption of the intended value where excel has converted what it's seeing into a date, which has in turn represented as a datetime object in Python. There is an obvious pattern so the \n",
        "I originally intended to replace the values as they should have been for that field, however the automated encoding will treat all of the dates as unique values and assign a value as they would if I manually replaced them.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAPN9JBnozsm"
      },
      "source": [
        "**Create any data pre-processing that you will conduct on seen and unseen data.  Regardless of the model you use, this dataframe must contain only numeric features and have a strategy for any expected missing values. Any objects can that are needed to handle the test data that are dependent on the training data can be stored in the model class.  You are recommended to use sklearn Pipelines or similar functionality to ensure reproduccibility.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdd_nO6cozsm"
      },
      "source": [
        "# Split your data so that you can test the effectiveness of your model\n",
        "X = data.iloc[:,0:9]\n",
        "y = data.iloc[:,-1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=42, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNdHTAvhE3fI",
        "outputId": "1978ccde-ce3b-4cb6-a68f-5f683ccc7ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Checking that the shapes of the datasets match. The model will not function with mismatched value counts.\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 9) (200,)\n",
            "(86, 9) (86,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsCwDzk1ozsq"
      },
      "source": [
        "# Populate preprocess_training_data and preprocess_test_data to preprocess data.\n",
        "# You nmust process test and train separately so your model does not accidently gain information that a model wouldnt have in reality and therefore get better predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_6uvjRkozsu"
      },
      "source": [
        "class Module4_Model:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        \n",
        "    def preprocess_training_data(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        This function should process the training data and store any features required in the class\n",
        "        \"\"\"\n",
        "            \n",
        "        #categorical_mask = train.dtypes==object\n",
        "        #cols = train.columns[categorical_mask].tolist()\n",
        "        processed_X_train = X_train.copy()\n",
        "        y_train.replace({'Class':{'recurrence-events':1,'no-recurrence-events':0}}, inplace=True)\n",
        "        processed_X_train.replace({'irradiat':{'yes':1, 'no':0}}, inplace=True)\n",
        "        \n",
        "        #newcol=train['Class'].copy()\n",
        "        processed_X_train=pd.get_dummies(processed_X_train)\n",
        "        #processed_train['Class']=newcol\n",
        "\n",
        "        return processed_X_train, y_train\n",
        "\n",
        "    def preprocess_test_data(self, X_test, y_test):\n",
        "    \n",
        "        #categorical_mask = test.dtypes==object\n",
        "        #cols = test.columns[categorical_mask].tolist()\n",
        "        processed_X_test = X_test.copy()\n",
        "        y_test.replace({'Class':{'recurrence-events':1,'no-recurrence-events':0}}, inplace=True)\n",
        "        processed_X_test.replace({'irradiat':{'yes':1, 'no':0}}, inplace=True)\n",
        "        \n",
        "        #newcol=test['Class'].copy()\n",
        "        processed_X_test = pd.get_dummies(processed_X_test)\n",
        "        #processed_test['Class']=newcol\n",
        "\n",
        "        return processed_X_test, y_test\n",
        "\n",
        "    def process_for_model(self, x_train_processed,x_test_processed):\n",
        "        # I opted for this option here to ensure that the test data has the same columns as the training data. This was an issue using this method. \n",
        "        # I only did it this way to fit with the class model that the challenge prescribed. I've approached a different way later to show how I intended \n",
        "        # to do it without this template.\n",
        "        x_test_processed = x_test_processed.reindex(columns = x_train_processed.columns, fill_value=0)\n",
        "        #X_train = x_train_processed.drop('Class', axis = 1)\n",
        "        #y_train = x_train_processed.Class\n",
        "        #X_test = x_test_processed.drop('Class', axis = 1)\n",
        "        #y_test = x_test_processed.Class\n",
        "\n",
        "        return x_train_processed, x_test_processed\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pX2YH1Yozsy"
      },
      "source": [
        "# Dont change\n",
        "my_model = Module4_Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTCV2JqXozs3"
      },
      "source": [
        "# Dont change\n",
        "x_train_processed, y_train = my_model.preprocess_training_data(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY6f8aLsGyLU",
        "outputId": "d5757b2d-ffd0-4b6a-b494-51c6f276acce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_train_processed.shape, y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 39) (200,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjCv3-wJoztD"
      },
      "source": [
        "# Dont change\n",
        "x_test_processed, y_test = my_model.preprocess_test_data(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaRPsNhlUckO"
      },
      "source": [
        "# Ensuring both sets are the same shape\n",
        "X_train,X_test = my_model.process_for_model(x_train_processed, x_test_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1vdzh0eEoRx",
        "outputId": "046c90fb-3e12-49d3-f6a7-c63d7b5bb929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 39) (200,)\n",
            "(86, 39) (86,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToPAJPS1ozs7"
      },
      "source": [
        "# Create a model\n",
        "# I chose the Random Forest Classifier. The first run with standard parameters\n",
        "random_forest = RandomForestClassifier(n_estimators=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7_YWBCdoztG",
        "outputId": "fe03a9b8-b77b-4f64-e5c5-bd61b9cf3ef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Train your model\n",
        "random_forest.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em71URrJoztO",
        "outputId": "d64182fc-20bc-4042-c09f-314457fe834b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# use your model to make a prediction on unseen data\n",
        "y_pred = random_forest.predict(X_test)\n",
        "rf_acc = round(accuracy_score(y_test,y_pred),4)*100\n",
        "print(f'{rf_acc}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72.09%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_7hMi1rajbR"
      },
      "source": [
        "#At this point conduct randomized search cross validation to get a better grip on the parameters. Then switch to the other model before doing the same\n",
        "# Some trial and error might make it better. Then deomstrate the pipeline version before moving toward NN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW06UWA9W4py",
        "outputId": "69095be3-ef23-4aad-f596-9cd43a2dc584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# run cross validation to get a mean idea of the accuracy\n",
        "rfc_cv_score=cross_val_score(random_forest, X_train, y_train, cv=10, scoring='roc_auc')\n",
        "# Cross validation seems to show a large variance in the accuracy of this model with a mean rouhgly where I expected\n",
        "print(f'The accuracy scores for the model over 10 runs:--\\n {rfc_cv_score}\\n')\n",
        "print(f'The mean Cross Validation Score for the model:--\\n{round(rfc_cv_score.mean()*100,2)}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy scores for the model over 10 runs:--\n",
            " [0.46428571 0.79761905 0.82142857 0.64880952 0.73809524 0.44047619\n",
            " 0.69047619 0.79761905 0.45833333 0.54945055]\n",
            "\n",
            "The mean Cross Validation Score for the model:--\n",
            "64.07%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALFww931LgST"
      },
      "source": [
        "*I used randomised search cross validation to assess possible better parameters for random forest*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftVxOo9mdAWD"
      },
      "source": [
        "# I redefine the variables to avoid crossover.\n",
        "\n",
        "X = data.iloc[:,0:9]\n",
        "y = data.iloc[:,-1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=42, shuffle=True)\n",
        "\n",
        "x_train_processed, y_train = my_model.preprocess_training_data(X_train, y_train)\n",
        "\n",
        "x_test_processed, y_test = my_model.preprocess_test_data(X_test, y_test)\n",
        "\n",
        "X_train,X_test = my_model.process_for_model(x_train_processed, x_test_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbFJ0akzLelq"
      },
      "source": [
        "# number of trees in a random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
        "# number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# maximum number of levels in the tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
        "# minimum number of samples to split a node\n",
        "min_samples_split = [2,5,10]\n",
        "# min samples required at each leaf node\n",
        "min_samples_leaf = [1,2,4]\n",
        "# method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid={'n_estimators':n_estimators,\n",
        "             'max_features':max_features,\n",
        "             'max_depth':max_depth,\n",
        "             'min_samples_split':min_samples_split,\n",
        "             'min_samples_leaf':min_samples_leaf,\n",
        "             'bootstrap':bootstrap}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15EaTN0aLqmW"
      },
      "source": [
        "rf_random = RandomizedSearchCV(estimator=random_forest, param_distributions = random_grid, n_iter=100, cv=3, verbose=2, n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z8dBM0dLrTp"
      },
      "source": [
        "rf_random.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPdW8olxNmSu",
        "outputId": "6c657d84-2d7a-4784-b6ec-6aa0c65003b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# This shows the best parameters based on the variations tried.\n",
        "best_fit = rf_random.best_params_\n",
        "best_fit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'max_depth': 80,\n",
              " 'max_features': 'auto',\n",
              " 'min_samples_leaf': 2,\n",
              " 'min_samples_split': 2,\n",
              " 'n_estimators': 1400}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvNAql1uN4tp"
      },
      "source": [
        "random_cv = RandomForestClassifier(n_estimators=1400, max_depth=80, max_features='auto', min_samples_leaf=2,min_samples_split=2, bootstrap=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIwyaB9cOBYl"
      },
      "source": [
        "random_cv.fit(X_train,y_train)\n",
        "# Run against unseen test data\n",
        "y_pred = random_cv.predict(X_test)\n",
        "random_rfacc = accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8G_iVZ4OFP4",
        "outputId": "763268bd-701b-4abb-f423-18f98484a24a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'Accuracy using the Randomized Search setting identified is {round(random_rfacc,4)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy using the Randomized Search setting identified is 74.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvgxFnCDb6BI",
        "outputId": "375edb2d-82b3-42db-ccf2-fbcd9d7bcabe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[57  5]\n",
            " [17  7]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7441860465116279"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0szC2L4oztV"
      },
      "source": [
        "# Asssess the accuracy of your model and explain your key findings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HxyZ0z2w0z0"
      },
      "source": [
        "*The accuracy of the model is of a reasoable standard, however there are variations of ~10% during different efforts at trainig. Cross validation shows even wider variation, from as low as the 40th percetile up to 80th+ on occasion. The most common accuracy is around 70% with the main hit to the accuracy coming from a high number of false negatives, that is, expectation of no recurrence where in reality re-occurrence was present.*\n",
        "\n",
        "*My belief is that the accuracy would be increased by a higher sample number, 286 is very low, and also a higher number of categories that contribute to the target variable. No amount of tinkering with the hyperparameters can overcome this shortfall.*\n",
        "\n",
        "*Factors I considered may skew data is age. If data provided to models of this nature, for this purpose, has individuals who are already more advanced in age the likelihood of seeing re-occurence is lower than that for a younger person.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnEPvNMIyIda"
      },
      "source": [
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCkrmYUnNiWW"
      },
      "source": [
        "*I wanted to attempt a different encoding method to see if the result is different in terms of accuracy*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCNTRQmTL-xJ"
      },
      "source": [
        "# For this method I need to ensure the values are all string values.\n",
        "data.loc[:, 'tumor-size'] = data.loc[:, 'tumor-size'].map(lambda x:str(x))\n",
        "data.loc[:, 'inv-nodes'] = data.loc[:, 'inv-nodes'].map(lambda x:str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo_LrcfvMTAf"
      },
      "source": [
        "# Again, redefine the variables to avoid crossover\n",
        "X = data.iloc[:,0:9]\n",
        "y = data.iloc[:,-1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=42, shuffle=True)\n",
        "\n",
        "x_train_processed, y_train = my_model.preprocess_training_data(X_train, y_train)\n",
        "\n",
        "x_test_processed, y_test = my_model.preprocess_test_data(X_test, y_test)\n",
        "\n",
        "X_train,X_test = my_model.process_for_model(x_train_processed, x_test_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwusHi2cozte"
      },
      "source": [
        "le_data = data.copy()\n",
        "categorical_mask = le_data.dtypes==object\n",
        "categorical_cols = le_data.columns[categorical_mask].tolist()\n",
        "le=LabelEncoder()\n",
        "le_data[categorical_cols]=le_data[categorical_cols].apply(lambda col:le.fit_transform(col))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir8kkpRaBkNC"
      },
      "source": [
        "leX = le_data.iloc[:, 0:9]\n",
        "ley = le_data.iloc[:, -1]\n",
        "leX_train, leX_test, ley_train, ley_test = train_test_split(leX,ley, test_size = 0.3, random_state=42, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL2w2aBsNn_H",
        "outputId": "063b46b6-82ad-41eb-a6ae-3f1d3a944c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "le_rfc = RandomForestClassifier(n_estimators=100)\n",
        "le_rfc.fit(leX_train,ley_train)\n",
        "le_prediction = le_rfc.predict(leX_test)\n",
        "print(f'Score using this method {round(le_rfc.score(leX_test,ley_test),3)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score using this method 70.89999999999999%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBjaQsejSwYU",
        "outputId": "55bafdcf-6869-487e-92b8-5337a566ce4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "le_rfc = cross_val_score(le_rfc, leX_train,ley_train, cv=10, scoring='roc_auc')\n",
        "print(f'The mean score for cross validation using the label encoder instead of Pandas Dummy values is {round(le_rfc.mean(),3)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean score for cross validation using the label encoder instead of Pandas Dummy values is 62.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sERKA56PUCfJ"
      },
      "source": [
        "*No real difference usin the Label Encoder instead of dummy values*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eBv3KlyUITX"
      },
      "source": [
        "*This is the method I'd prepared before looking at the template: I think it works better for the user and probably deals with the potential for irregular data better.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQSqYH-Y7L1t"
      },
      "source": [
        "# Define transformers for each data type with methods for handling unkown and missing data\n",
        "# In the numeric_transformer I've opted for the most_frequent option of filling as that is my general method of handling missing data when doing it manually.\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "numeric_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = data.select_dtypes(include=['object']).drop(['Class'], axis=1).columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', RandomForestClassifier())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtofJ5inZZba",
        "outputId": "7c97cb17-dd98-4d23-e3cc-64a5bf39ad56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Split and create the sample data for this method\n",
        "pip_X = data.iloc[:,0:9]\n",
        "pip_y = data.iloc[:, -1]\n",
        "pip_X_train, pip_X_test, pip_y_train, pip_y_test = train_test_split(pip_X,pip_y, test_size=0.3, random_state=42, shuffle=True)\n",
        "\n",
        "print(pip_X_train.shape, pip_y_train.shape)\n",
        "print(pip_X_test.shape, pip_y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 9) (200,)\n",
            "(86, 9) (86,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUydtjFl9TLa",
        "outputId": "ccf5735c-db56-42c2-af21-fc4076f44d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "rf.fit(pip_X_train,pip_y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('num',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('imputer',\n",
              "                                                                   SimpleImputer(add_indicator=False,\n",
              "                                                                                 copy=True,\n",
              "                                                                                 fill_value=None,\n",
              "                                                                                 missing_values=nan,\n",
              "                                                                                 strategy='most_frequent',\n",
              "                                                                                 verbose=0)),\n",
              "                                                                  ('scaler',\n",
              "                                                                   StandardScaler(copy=True,\n",
              "                                                                                  wi...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HR68UolXja-",
        "outputId": "66c496ab-5c98-4c1a-ddff-dd1bb48e26b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pip_y_pred = rf.predict(pip_X_test)\n",
        "\n",
        "pip_rf_acc = round(rf.score(pip_X_test,pip_y_test)*100,2)\n",
        "\n",
        "print(f'Accuracy of the pipeline method is {pip_rf_acc}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the pipeline method is 73.26%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P_5fddMbuNz",
        "outputId": "d36079cc-ca7d-48c6-fc82-2395925b31b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "cm = confusion_matrix(pip_y_test, pip_y_pred)\n",
        "print(cm)\n",
        "accuracy_score(pip_y_test, pip_y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[51 11]\n",
            " [14 10]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7093023255813954"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMc4JNSAXrfg"
      },
      "source": [
        "# run cross validation to get a better overview of the scores\n",
        "rfc_cv_score=cross_val_score(rf, pip_X_train, pip_y_train, cv=10, scoring='roc_auc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOroFHfIXuXV",
        "outputId": "49cd8c39-33d8-42c8-cc93-04fc6c5ce2a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Cross validation seems to show a large variance in the accuracy of this model with a mean rouhgly where I expected\n",
        "print(f'The scores for the model over the 10 runs of the model:--\\n {rfc_cv_score}\\n')\n",
        "print(f'The mean Cross Validation Score for the model:--\\n{round(rfc_cv_score.mean()*100,2)}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The scores for the model over the 10 runs of the model:--\n",
            " [0.45238095 0.7797619  0.86309524 0.64880952 0.73809524 0.53571429\n",
            " 0.68452381 0.88095238 0.4047619  0.53846154]\n",
            "\n",
            "The mean Cross Validation Score for the model:--\n",
            "65.27%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTu7ldpaf8Oy"
      },
      "source": [
        "*My attempt at an Artifical Neural Network*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTRelt28f-T6"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cDYvq_FB7zg"
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ5GqaNAgRc_",
        "outputId": "a2b1b6a9-9328-4a88-aa7c-3389ced7a416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MNZCJHYhPed"
      },
      "source": [
        "dataset = data.copy()\n",
        "X = dataset.iloc[:, 0:9].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCKCvjNvjI7G"
      },
      "source": [
        "#encode each categorical column. I've done all but degmalig\n",
        "le=LabelEncoder()\n",
        "cols = [0,1,2,3,4,6,7,8]\n",
        "for col in cols:\n",
        "    X[:, col] = le.fit_transform(X[:, col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIrVQZpjqlwb"
      },
      "source": [
        "y = le.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4OgY6kflsVN"
      },
      "source": [
        "#split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4o2qrz8mvAQ"
      },
      "source": [
        "#apply feature scaling to all the data...this is absolutely imperative to deep learning\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RN9S0Oencd3"
      },
      "source": [
        "#initialising the ANN\n",
        "#creating a sequence of layers as opposed to computational graph\n",
        "ann = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oN4zxdBpRiF"
      },
      "source": [
        "*adding the input layer and first hidden layer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxxy_u9BoA5x"
      },
      "source": [
        "#add a fully connected layer to the NN. This can be done at any phase\n",
        "ann.add(tf.keras.layers.Dense(units=6, activation = \"relu\")) #rectifier activation function = relu\n",
        "# units is important and designates the number of hidden neurons\n",
        "# there is no way to know how many neurons we want. there is no rule, it's based on experimentation in tweaking the hyper parameters before training the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnJgsNKYpU4k"
      },
      "source": [
        "#add the second layer. This is exactly the same as adding the first\n",
        "ann.add(tf.keras.layers.Dense(units=6, activation = \"relu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-v938G9pjjF"
      },
      "source": [
        "*adding the output layer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5FXuvaApk5t"
      },
      "source": [
        "#mostly the same as we're adding a new layer again, just slightly different settings as this is the output layer\n",
        "# because we're doing classiication on a binary target variable we only need one unit/neuron, sigmoid activation function for predictions and probability\n",
        "#of our vaiable function...this is only for the output layer. this shows the probability of the outcome rather than yes/no or 1/0\n",
        "ann.add(tf.keras.layers.Dense(units = 1, activation = \"sigmoid\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbYKs1h9rbSH"
      },
      "source": [
        "*compiling the ANN*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paQMOPelq-5Z"
      },
      "source": [
        "#loss computes the prediction between the prediction and the real result\n",
        "# if conducting loss on a binary classification you must use the following loss method. for none binary it should be categorcal_crossentropy and\n",
        "#in activation for non-binary on the exit layer the activation should be \"cross max\"\n",
        "ann.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) #metrics can take multiiple parameters so you enter in [] like a list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taXqz_H-sr7k"
      },
      "source": [
        "*training the ANN*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44pm4GiNspjL"
      },
      "source": [
        "#batch size is commonly 32. training is conducted over a number of epochs which you also must define\n",
        "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyx55caJm2H_"
      },
      "source": [
        "*predicting the test results*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXpM0RUCtawV"
      },
      "source": [
        "y_pred = ann.predict(X_test)\n",
        "y_pred = (y_pred >0.5)\n",
        "#shows the predicted results against the actual test results\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ7rUwUdm8ea"
      },
      "source": [
        "*create confusion matrix to show actual accuracy of the network*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWh4_0eHmz49",
        "outputId": "58ac0e84-9485-42bf-8783-7b333376d3d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)\n",
        "# Not a great outcome. Commonly it seems we get a false negative where the network predicts no recurrence when in fact some occurs."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[54  0]\n",
            " [32  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.627906976744186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8un7zMa0uodi"
      },
      "source": [
        "*Change the learning rates*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEdt8XF2BnbU"
      },
      "source": [
        "# redefine the samples\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFQcI4-6unhB"
      },
      "source": [
        "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n",
        "\n",
        "\n",
        "    def schedule(epoch):\n",
        "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "lr_sched = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)\n",
        "\n",
        "ann.fit(X_train, y_train, batch_size=32, epochs=100, callbacks=[lr_sched])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAcbyqzqE_lO"
      },
      "source": [
        "#7.5510e-11\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0000000000755)\n",
        "ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SU4gsilFmAo"
      },
      "source": [
        "ann.fit(X_train, y_train, batch_size=32, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w7kbZgWF2KX",
        "outputId": "85e6332d-3d7e-41eb-fa34-c5d028bdc204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = ann.predict(X_test)\n",
        "y_pred = (y_pred>0.5)\n",
        "print(f'The ANN produced an accuracy score of {round(accuracy_score(y_test, y_pred),3)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ANN produced an accuracy score of 65.10000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEIVyDexbRDF"
      },
      "source": [
        "*As with the ML model there is not a huge degree of accuracy, despite efforts to narrow down the hyperparameters. I believe this to be down to the sample size of the original data and the number of contributory categories within.*"
      ]
    }
  ]
}